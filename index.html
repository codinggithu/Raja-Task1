<html>
<head>
    <title>Task</title>
    <link rel="stylesheet" href="style.css">
    <script src="main.js"></script>
</head>
    
    <body>
        <center>
            <p><b>PAGES:256-260</b></p></center>
    
     <p class id="p1" >best or worst performance periods. Some of the same subjects as noted above for cross-<br>sectional studies may be worthwhile targets. For example, it would be of interest for quality<br>
assurance to determine the correlates of best and worst defect production rates.<br>
 <br>                Ordinary regression is one of the most important tools for data mining. Frontier models,<br>
such as considered here, may be desirable alternatives in connection with data-mining<br>
applications. This is especially the case when it is desired to characterize and model the best<br>
and/or worst cases in the data. Such data are typically of the managed kind. In general, such<br>
managed data or data from purposeful or goal-directed behavior will be amenable to frontier<br>
modeling.
</p>
    <h2>CONCLUSIONS</h2>
    
        
        <p class id="p2">This chapter proposes a method for estimating what may be called benchmark, or<br>
best practice unit and marginal cost rates. These rates provide plausible operational<br>
goals for the management of the units being compared. This method also provides<br>
efficiency measures and suggests which organizational units or time periods are more or<br>
less efficient, as well as an estimate of the degree of such inefficiency. Efficient units or<br>
time periods provide benchmarks for imitation by other units or can be studied for<br>
continuous improvement possibilities. As far as the authors can determine, the proposed<br>
methodology is the first technique with the capability to suggest plausible benchmark<br>
cost rates.<br>
  <br>      A principle of maximum performance efficiency (MPE) was proposed as a generali-<br>zation of the maximum decisional efficiency estimation principle in Troutt (1995). This<br>
principle is more broadly applicable than the MDE principle. Also, a gamma distribution-<br>based validation criterion was proposed for the new MPE principle. The earlier MDE<br>
principle appealed to the maximum likelihood estimation principle for model aptness<br>
validation, but required relatively inflexible density models for the fitted efficiency<br>
scores.<br>
        
    <br> The estimation models derived here reduce to straightforward linear programming<br>
models and are therefore widely accessible. A case was made that an optimal cost rate<br>
estimate of zero for some activity may be indicative of generally poor efficiency across<br>
the units with respect to one or more activities. Modifications based on reduced costs<br>
and augmented objective functions were proposed to compensate in that case.<br>
    <br>In these models, the unknown cost rate parameters resemble the coefficients of<br>
linear regression models. However, the maximum performance efficiency estimation<br>
principle is employed rather than a criterion such as ordinary least squares. This principle<br>
assumes that for each organizational unit and time period, the unit intends to minimize<br>
these costs. Model adequacy with respect to this assumption was judged by a test of<br>
normal-like-or-better performance effectiveness for the estimated efficiency scores.<br>
<br>    These results are also consistent with Noreen and Soderstrom (1994) who found<br>
that costs were not generally proportional to activity levels in a cross-sectional study<br>
of hospital accounts. The results of this chapter suggest that one source of such non-<br>proportionality, in addition to the possibly non-linear form of the basic cost model, is<br>
what we call performance inefficiency</p>
    <br>
        <p class id="p3">The proposed estimation criterion was applied to a published data set previously<br>
analyzed by a modified data envelopment analysis method. The resulting estimates were<br>
compared with the average costs obtained by the previous method. The estimated<br>
benchmark cost rates were uniformly and strictly lower than their average rate counter-<br>parts, consistent with their definitions and providing a strong measure of face validity.<br>
   <br>  Benchmarking estimation models, such as discussed here, provide a new tool for<br>
data mining when the emphasis is on modeling the best performers.</p>
    <h2>REFERENCES</h2>
        <pre>Berson, A., Smith, S., & Thearling, K (2000). Building data mining applications for CRM.<br>
New York: McGraw-Hill.<br>
BestFit™(1995). User’s guide. Palisade Corporation, Newfield, NY.<br>
Bickell, P.J. & Doksum, K.A. (1977). Mathematical statistics: Basic ideas and selected<br>
topics. San Francisco: Holden Day, Inc.<br>
Charnes, A., Cooper, W.W., Lewin, A., & Seiford, L.M. (1994). Data envelopment<br>
analysis: Theory, methodology, and applications. Boston, MA: Kluwer Academic<br>
Publishers.<br>
Cooper, R. & Kaplan, R.S. (1992). Activity-based systems: Measuring the costs of<br>
resource usage. Accounting Horizons, September, 1-13.<br>
Datar, S. & Gupta, M. (1994). Aggregation, specification and measurement errors in<br>
product costing. The Accounting Review, 69(4), 567-591.<br>
Demski, J. (1967). An accounting system structured on a linear programming model. The<br>
Accounting Review, 42(4), 701-712.<br>
Devroye, L. (1986). Non-uniform random variate generation. New York: Springer-Verlag.<br>
Dyson, R.G. & Thanassoulis, E. (1988). Reducing weight flexibility in data envelopment<br>
analysis. Journal of the Operational Research Society, 39(6), 563-576.<br>
Fleming, W. (1997). Functions of several variables, 2nd ed. New York: Springer-Verlag.<br>
Itami, H. & Kaplan, R. (1980). An activity analysis approach to unit costing with multiple<br>
interactive products. Management Science, 26(8), 826-839.<br>
Kaplan, R. & Thompson, G. (1971). Overhead allocation via mathematical programming<br>
models. The Accounting Review, 46(2), 352-364.<br>
Kotz, S., Fang, K.T., & Liang, J.T. (1997). On multivariate vertical density representation<br>
and its application to random number generation. Statistics, 30, 163-180.<br>
Kotz, S. & Troutt, M.D. (1996). On vertical density representation and ordering of<br>
distributions. Statistics, 28, 241-247.<br>
Law, A.M. & Kelton, W.D. (1982) Simulation modeling and analysis. New York:<Br>
McGraw-Hill.<br>
Madansky, A. (1988). Prescriptions for working statisticians., New York: Springer-Verlag.<br>
Neter, J., Wasserman, W., & Kutner, M.H. (1985). Applied linear statistical models, 2nd<br>
ed. Homewood, IL: Richard E. Irwin, Inc.<br>
Noreen, E. & Soderstrom, N. (1994). Are overhead costs strictly proportional to activity?<br>
Journal of Accounting and Economics, 17, 255-278Onsi, M. (1970). Transfer pricing systems<br> based on opportunity cost. The Accounting<br>
Review, 40(3), 535-543.<br>
SAS/IML Software (1995). Usage and reference. Version 6, 1st ed., SAS Institute, Inc.,<br>
Cary, N. C.<br>
Schmeiser, B.W. & Lal, R. (1980). Squeeze methods for generating gamma variates.<br>
Journal of the American Statistical Association, 75, 679-682.<br>
Thannassoulis, E., Dyson, R.G., & Foster, M.J. (1987). Relative efficiency assessments<br>
using data envelopment analysis: An application to data on rates departments.<br>
Journal of the Operational Research Society, 38(5), 397-411.< br>
Troutt, M.D. (1991). A theorem on the density of the density ordinate and alternative<br>
derivation of the Box-Muller method. Statistics, 22, 436-466.<br>
Troutt, M.D. (1993). Vertical density representation and a further remark on the Box-<br>Muller method. Statistics, 24, 81-83.
Troutt, M.D. (1995). A maximum decisional efficiency estimation principle, Management<br>
Science, 41(1), 76-82.<br>
Troutt, M.D. (1997). Derivation of the maximum efficiency ratio model from the maximum<br>
decisional efficiency principle. Annals of Operations Research, 73, 323-338.<br>
Troutt, M.D., Gribbin, D.W., Shanker. M., & Zhang. A. (2000). Cost-efficiency<br>
benchmarking for operational units with multiple cost drivers. Decision Sciences,<br>
31(4), 813-832.<br>
Troutt, M.D., Hu, M., Shanker, M. & Acar, W. (2001). Frontier versus ordinary regression<br>
models for data mining. In Parag C. Pendharkar (ed.), Managing data mining<br>
technologies in organizations: Techniques and applications. Hershey, PA: Idea<br>
Group Publishing Co.<br>
Troutt, M.D. & Pang, W.K. (1996). A further VDR-type density representation based on<br>
the Box-Muller method. Statistics. 28, 1-8.<br>
Troutt, M.D., Zhang, A., Tadisina, S.K., & Rai, A. (1997). Total factor efficiency/<br>
productivity ratio fitting as an alternative to regression and canonical correlation<br>
models for performance data. Annals of Operations Research, 74, 289-304..<br>
        </Br></pre>
        <h2>APPENDIX</h2>
<h3> Proof of Theorem 1:</h3>
<img src="Task-img1.jpeg"  width=250px height=200px>
<p class id="p4">(2) Here we note that the ar
*(λ) for the MMPE model, while feasible, are not necessarily<br>
optimal for the original MPE model. Hence use of these values in the MPE objective<br>
function will generally give a lower objective function value></p>
<br>
<h3>Proof of Theorem 2:</h3>
<p class id="p5">This is a modification of a proof for a version of the theorem given in Troutt (1993). By the<br>
assumption that x is uniformly distributed on {x:w(x) = u}, f(x) must be constant on these<br>
contours; so that f(x) = ϕ(w(x)) for some function, ϕ(⋅) . Consider the probability P( u ≤ w(x)<br>
≤ u + ε ) for a small positive number, ε. On the one hand, this probability is ε g(u) to a first<br>
order approximation. On the other hand, it is also given by<br></p>
<br>
<img src="Task-img2.jpeg" width=250px height=200px>
<h3>Further Details on the Simulation Experiment</h3>
<p class id="p6">To simulate observations within each data set, a uniform random number was used<br>
to choose between the degenerate and continuous portions in the density model<br>
<br>
p δ(0) + (1-p) gamma(α,β)<br>
<br>
where p =0.048, α=1.07, and β=0.32. With probability p, δ(0) was chosen and w=0 was returned.<br>
With probability 1-p, the gamma (α,β) density was chosen and a value, w, was returned using<br>
the procedure of Schmeiser and Lal (1980) in the IMSL routine RNGAM. The returned w was<br>
converted to an efficiency score, v, according to v=exp{-w0.5}. For each v, a vector Y was<br>
generated on the convex polytope with extreme points e1
=(v/a1
*,0,0,0), e2
=(0,v/a2
*,0,0),<br>
e3
=(0,0,v/a3
*,0) and e4
=(0,0,0,v/a4
*) using the method given in Devroye (1986).<br></p>
    <br>
     
         <h4>Chapter XI </h4>
  <h1>Bayesian Data Mining and<br> Knowledge Discovery</h1>
    <center><p class id="p7">Eitel J.M.Lauria<br>State University of New York,Albany,USA<br>Universidad delSalvador,Argetina<br>
        <br>Giri Kumar Tayi<br>State University of New York,Albany,USA</p></center><br>
    <br>
    <h2>ABSTRACT</h2><br>
    <p class id="p8">One of the major problems faced by data-mining technologies is how to deal with<br>
uncertainty. The prime characteristic of Bayesian methods is their explicit use of<br>
probability for quantifying uncertainty. Bayesian methods provide a practical method<br>
to make inferences from data using probability models for values we observe and about<br>
which we want to draw some hypotheses.
        Bayes’ Theorem provides the means of<br>
calculating the probability of a hypothesis (posterior probability) based on its prior<br>
probability, the probability of the observations, and the likelihood that the observational<br>
data fits the hypothesis.<br>
The purpose of this chapter is twofold: to provide an overview of the theoretical<br>
framework of Bayesian methods and its application to data mining, with special<br>
emphasis on statistical modeling and machine-learning techniques; and to illustrate<br>
each theoretical concept covered with practical examples. We will cover basic<br>
probability concepts, Bayes’ Theorem and its implications, Bayesian classification,<br>
Bayesian belief networks, and an introduction to simulation techniques.</p>
</body>
<style>
    p{  
      margin-left: 350px;
      margin-right: 300px;
    
     }
    h1{
        margin-left: 450px;
    }
    h2{
        margin-left: 500px;
    
    }
    h3{
       margin-left: 350px;
    }
    h4{
        margin-left: 550px;
    }
    
    p8{
        font-family: cursive;
    }
        pre{
            margin-left: 350px;
        }
    
    img{
        margin-left: 350px;
    }
    </style>

</html>